# Lab1 MapReduce

MapReduce 是一套比较经典而且简单的分布式计算框架了，虽然我已经大概了解 MapReduce 原理快两年了，但实际上手，用代码写一个简易的 MapReduce 还是遇到不少坑，有很多细节要注意的。

2020 年 MIT 6.824 的课程 lab 和 2018 虽然内容相同，但代码结构有很大的变化，所以下面是我完成 2020 年 lab1 的思路过程。

## Worker 

在写第一行代码之前，我犹豫了很久，需不需要一个 for 循环维持 worker 函数一直执行。后来我看了一眼 2018 年 lab1 的代码，发现逻辑发生了很大的变化。2018 年 worker 只需要执行 task 就行了，由统一的 shedule 负责调用，开启一个新的协程执行，所以 worker 不需要循环。而 2020 年的逻辑更符合实际的 MapReduce，需要手动开启一个 worker 进程，自己请求 master 任务，所以最后我采用了循环。

在启动 worker 时，需要先向 master 进行注册，来获取一个唯一的 workerId。在一开始，我为了省事，直接分配一个随机数。但后来并发同时启动多个 worker 时，就出现了 bug，多个 worker 具有相同的 ID。因为我是以时间为随机种子进行随机，而 time 只能精确到毫秒级。这就导致同时开启多个进程时，随机种子是相同的。所以，最后我还是采用了注册的方式。

master 和 worker 之间通过 RPC 进行通信。worker 请求任务时，发送 workerId 和请求类型（请求任务或确认完成任务），获取任务类型、任务编号、文件和 reduce 的数量。虽然reduce 的数量是在 master 初始化时赋值的，但我不确定后续会不会对这个值进行修改，所以每次请求都选择传一次。

首先，为了防止 worker crash掉后，未完全完成的中间文件被读取，所以先创建 tmp 文件，完成任务后，再改为中间文件。

这里有一点比较坑的地方，在创建新文件，如果文件已存在，会出现两种情况。如果创建新文件且写入内容，则旧文件会被覆盖，但如果没有写入，旧文件内容依然存在。

任务完成后，worker 向 master 发送确认请求。

Reduce 的过程和 Map 类似，但 Reduce 的输出都是写入一个 mp-out 文件。

在整个 job 完成后，worker 仍会继续请求 master，但由于 master 已经关闭了，所以请求会失败，这就可以当做判断的依据了。我实现的逻辑就是如果 worker 请求 master 失败两次，就结束进程。

## Master

master 的实现其实不是很顺利，也是根据测试一点点重构，修补的。后来想了想，还是因为想偷懒，很多 MapReduce 的机制都想简化掉，但到最后发现，还是要实现。

master 接收到 worker 的请求后，会根据不同的阶段，选择不同的输入文件，进行分配。同时，master 保存一个 map，所有正在执行的 worker-task对。当任务完成时，就从 map 中会删除键值对。在 Map 阶段，还需要额外保存有效的 task 编号。需要保存的原因就是根本上是连续执行多个 job 的问题。在 lab 的 test 脚本中，每个 test 的执行之间并没有删除中间文件，这也就导致最后 crash 测试，也是最符合实际情况的测试，Reduce 阶段可能会读取到上一个 job 遗留下来的中间文件。保存有效的 task 编号其实也是一种与实际的 MapReduce 保存中间文件位置类似的简化形式。

在分配任务的同时，会开启一个协程，负责记录任务执行的时间。如果超过最长时间（lab中为10秒），那么 master 就可以认为 worker 已经 crash 掉了。worker 正在执行的任务可以重新分配。

当 worker 请求任务，但是当前 master 在等待其他任务完成才能进入 Reduce 阶段或者结束时，master 会返回给 worker 特殊的状态码，worker 都等待一段时间，再进行请求。

master 保存着所有正在执行任务的记录，所以当记录列表和未输入文件列表都为空，即可以判断进入 Reduce 阶段，当未分配任务列表和记录列表都为空时，即可以判断 job 结束。

